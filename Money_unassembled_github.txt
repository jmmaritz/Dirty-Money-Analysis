

## Shotgun metagenomic data - Unassembled Workflow

## Data analysis workflow used in manuscript:
## Filthy lucre: A metagenomic pilot study of microbes found on circulating currency in New York City
## Maritz, J.M., Sullivan, S.A., Prill, R.J. et al. PLOS ONE (2017) 12: 4.

## All data analysis carried out on New York University's High Performance Compute cluster
## Parameters and file paths may vary on your system
## Edit commands as necessary to replicate these data analysis locally



############### Trimming of Adapter Sequences and Read Filtering ##############

## The adapters were Bio Scientific NEXTflex-96 DNA Barcodes
## Both the P5, P7 and their reverse compliments were included in fasta format for adapter trimming

#!/bin/bash

#PBS -q cgsb-s
#PBS -l nodes=1:ppn=4,walltime=12:00:00,mem=20000mb
#PBS -N Flexbar
#PBS -m a
#PBS -e localhost:${PBS_O_WORKDIR}/${PBS_JOBNAME}.e${PBS_JOBID}

cd $PBS_O_WORKDIR

qstat -f $PBS_JOBID
echo `date`
echo `hostname`

PREFIX=Flexbar
LOG=${PREFIX}.log

if [ -f "$LOG" ]; then
        echo "error: log file [ $LOG ] exists"
        exit 1
fi

echo _BEGIN_ Flexbar `date` > $LOG

echo `set` >> $LOG

module load flexbar/intel/2.32

flexbar -r rawreads_R1.fastq -p rawreads_R2.fastq -a adapters.fasta -t AdaptersTrimmed -f fastq -n 8 -ao 4

flexbar -r AdaptersTrimmed_R1.fastq -p AdaptersTrimmed_R2.fastq -t FilteredReads -f fastq -n 8 -z 100 -m 90

# -ao minimum overlap of adapter and read
# -z trim to specified length from 3' end after adapter removal
# -m minimum read length to remain after adapter removal

exit 0;


############### Remove Human Sequences ##############

### Fix the headers of the filtered reads as Bowtie2 will truncate at the first whitespace

#!/bin/bash

#PBS -q cgsb-s
#PBS -l nodes=1:ppn=4,walltime=4:00:00,mem=8gb
#PBS -N nowhite
#PBS -m a 
#PBS -e localhost:${PBS_O_WORKDIR}/${PBS_JOBNAME}.e${PBS_JOBID}

cd $PBS_O_WORKDIR

perl -pe '/^@HWI/ && s/ /_/' FilteredReads_R1.fastq > FilteredReads_R1_nowhite.fastq

perl -pe '/^@HWI/ && s/ /_/' FilteredReads_R2.fastq > FilteredReads_R2_nowhite.fastq

exit 0;

### Align the reads to the human genome  with Bowtie2

#!/bin/bash

#PBS -q cgsb-s
#PBS -l nodes=1:ppn=12,walltime=12:00:00,mem=8gb
#PBS -N bowtie2
#PBS -M jmm1005@nyu.edu
#PBS -m a 
#PBS -e localhost:${PBS_O_WORKDIR}/${PBS_JOBNAME}.e${PBS_JOBID}
#PBS -o localhost:${PBS_O_WORKDIR}/${PBS_JOBNAME}.o${PBS_JOBID}

cd $PBS_O_WORKDIR

module load bowtie2/intel/2.1.0
module load samtools/intel/0.1.19

# preindexed HG19 can be downloaded from here: ftp://igenome:G3nom3s4u@ussd-ftp.illumina.com/Homo_sapiens/UCSC/hg19/Homo_sapiens_UCSC_hg19.tar.gz
# After unzipping (tar -zxvf Homo_sapiens_UCSC_hg19.tar.gz) BWA index of the whole genome found in: .../Annotation/Sequence/BWAIndex ...

# need to designate bowtie2 index file if not in current directory, see below

export BOWTIE2_INDEXES=/Homo_sapiens/UCSC/hg19/Sequence/Bowtie2Index/

bowtie2 -p 12 -x genome -1 FilteredReads_R1_nowhite.fastq -2 FilteredReads_R2_nowhite.fastq -S HumanAlignedReads.sam --phred33 -X 750 --local --reorder --met-stderr

samtools view -hS -F 4 HumanAlignedReads.sam > HumanReads.sam

samtools view -hS -f 4 HumanAlignedReads.sam > NonHumanReads.sam

# -F 4 skip alignments with a score of 4 (no reported alignment) = Only human aligned reads
# -f 4 only output alignments with a score of 4 = Only unaligned reads

exit 0;


############### Remove Exact Duplicates ##############

### Identify and remove exact duplicates to reduce dataset size

#!/bin/bash

#PBS -q cgsb-s
#PBS -l nodes=1:ppn=4,walltime=12:00:00,mem=8gb 
#PBS -N samtofasta
#PBS -m a 
#PBS -e localhost:${PBS_O_WORKDIR}/${PBS_JOBNAME}.e${PBS_JOBID}

cd $PBS_O_WORKDIR

# use Non-human sequences only and convert to a tabbed fasta file

grep -v ^@ NonHumanReads.sam | perl -pe 's/^/>/' | cut -f1,10 > tabbed_unsorted_nonhuman.fasta

# sort based on 2nd col, unique based on 2nd col, convert to 2 line fasta

sort -k 2,2 tabbed_unsorted_nonhuman.fasta | uniq -f 1 -u | perl -pe 's/\t/\n/g' > Uniqued_NonHuman.fasta

exit 0;


### The resulting file was used as input for a Blast search to assign taxonomy







